{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwAnnaX7eVvz"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKVmF0APRk7-"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "path_workspace = '/content/r2_learning_calibration'\n",
        "path_source = '/content/drive/MyDrive/RAVEN Calibration/experiments/recorded_trajs'\n",
        "sys.path.append(path_source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5BWVDa9Fk5z"
      },
      "outputs": [],
      "source": [
        "!mkdir '/content/r2_learning_calibration'\n",
        "!mkdir '/content/r2_learning_calibration/data'\n",
        "\n",
        "# !unzip -q \"/content/drive/MyDrive/RAVEN Calibration/experiments/recorded_trajs/record_1_different_directions.zip\" -d '/content/r2_learning_calibration/data'\n",
        "\n",
        "# !unzip -q \"/content/drive/MyDrive/RAVEN Calibration/experiments/recorded_trajs/record_2_diff_sparsity.zip\" -d '/content/r2_learning_calibration/data'\n",
        "\n",
        "!unzip -q \"/content/drive/MyDrive/RAVEN Calibration/experiments/recorded_trajs/record_3_time_decay_unloaded.zip\" -d '/content/r2_learning_calibration/data'\n",
        "\n",
        "# !unzip -q \"/content/drive/MyDrive/RAVEN Calibration/experiments/recorded_trajs/record_3_time_decay_idle.zip\" -d '/content/r2_learning_calibration/data'\n",
        "\n",
        "# !unzip -q \"/content/drive/MyDrive/RAVEN Calibration/experiments/recorded_trajs/record_3_time_decay_unloaded.zip\" -d '/content/r2_learning_calibration/data'\n",
        "\n",
        "# !unzip -q \"/content/drive/MyDrive/RAVEN Calibration/experiments/recorded_trajs/record_4_home_decay_no_home_in_training.zip\" -d '/content/r2_learning_calibration/data'\n",
        "\n",
        "# !unzip -q \"/content/drive/MyDrive/RAVEN Calibration/experiments/recorded_trajs/record_4_home_decay_with_home_in_training.zip.zip\" -d '/content/r2_learning_calibration/data'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDDRDTWPRvSJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "import heapq\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRpd_InUIkzA"
      },
      "outputs": [],
      "source": [
        "# parameter list\n",
        "use_short_vali = True # using short validation set can speed up training, but should no longer use validation to tune the hyperparameters\n",
        "combine_testset = False\n",
        "train_on_err = True\n",
        "save_rst = False  # if true, prediction result will be saved as csv\n",
        "\n",
        "# file_train_list_ = ['data_record_yz_05_4.csv', 'data_record_yz_025_4.csv']\n",
        "# # # file_train_list_ = ['data_record_rand_05_1.csv','data_record_rand_03_1.csv','data_record_rand_025_1.csv']\n",
        "# file_vali_list_ = ['data_record_rand1200_016.csv']\n",
        "# file_test_list_ = [ 'data_record_rand1200_05.csv', 'data_record_rand1200_025.csv']\n",
        "\n",
        "# EXP1 Different Direction\n",
        "# file_train_list_ = ['data_record_x_05.csv','data_record_x_03.csv','data_record_x_025.csv']\n",
        "# file_train_list_ = ['data_record_yz_05.csv','data_record_yz_03.csv','data_record_yz_025.csv']\n",
        "# # file_test_list_ = ['data_record_rand1200_x.csv', 'data_record_rand1200_y.csv', 'data_record_rand1200_x.csv','data_record_rand1200_x.csv', 'data_record_rand1200_x.csv']\n",
        "# file_vali_list_ = ['data_record_rand1200_yz.csv']\n",
        "# file_test_list_ = ['data_record_rand1200_yz.csv']\n",
        "\n",
        "# EXP2 Different Sparsity\n",
        "# # # file_train_list_ = ['data_record_yz_05_1.csv','data_record_yz_03_1.csv','data_record_yz_025_1.csv', 'data_record_yz_02_1.csv', 'data_record_yz_016_1.csv']\n",
        "# file_train_list_ = ['data_record_yz_05_5.csv', 'data_record_yz_02_3.csv']\n",
        "# # # file_test_list_ = ['data_record_rand1200_05.csv', 'data_record_rand1200_03.csv', 'data_record_rand1200_025.csv','data_record_rand1200_02.csv', 'data_record_rand1200_016.csv']\n",
        "# file_vali_list_ = ['data_record_rand1200_05.csv', 'data_record_rand1200_02.csv']\n",
        "# file_test_list_ = ['data_record_rand1200_05.csv', 'data_record_rand1200_02.csv']\n",
        "\n",
        "# EXP3 Time decay\n",
        "# file_train_list_ = ['data_record_yz_05_1.csv', 'data_record_yz_03_1.csv', 'data_record_yz_025_1.csv', 'data_record_yz_02_1.csv', 'data_record_yz_016_1.csv']\n",
        "file_train_list_ = ['data_record_yz_05_1.csv', 'data_record_yz_03_1.csv', 'data_record_yz_025_1.csv']\n",
        "file_vali_list_ = ['data_record_rand1200_h0.csv']\n",
        "file_test_list_ = ['data_record_rand1200_h0.csv', 'data_record_rand1200_h1.csv', 'data_record_rand1200_h2.csv', 'data_record_rand1200_h3.csv', 'data_record_rand1200_h4.csv', 'data_record_rand1200_h5.csv']\n",
        "\n",
        "# # file_train_list_ = ['data_record_yz_05.csv', 'data_record_yz_03.csv', 'data_record_yz_025.csv', 'data_record_yz_02.csv', 'data_record_yz_016.csv']\n",
        "# file_train_list_ = ['data_record_yz_03.csv', 'data_record_yz_02.csv']\n",
        "# file_vali_list_ = ['data_record_rand1200_idle_h0.csv']\n",
        "# file_test_list_ = ['data_record_rand1200_idle_h0.csv', 'data_record_rand1200_idle_h5.csv']\n",
        "\n",
        "# file_train_list_ = ['data_record_yz_05.csv', 'data_record_yz_03.csv', 'data_record_yz_025.csv', 'data_record_yz_02.csv', 'data_record_yz_016.csv']\n",
        "# file_train_list_ = ['data_record_yz_03.csv', 'data_record_yz_02.csv']\n",
        "# file_vali_list_ = ['data_record_rand1200_h0.csv']\n",
        "# file_test_list_ = ['data_record_rand1200_h0.csv', 'data_record_rand1200_h1.csv', 'data_record_rand1200_h2.csv', 'data_record_rand1200_h3.csv', 'data_record_rand1200_h4.csv', 'data_record_rand1200_h5.csv']\n",
        "\n",
        "# EXP4 Homing decay\n",
        "# file_train_list_ = ['data_record_yz_05.csv', 'data_record_yz_03.csv', 'data_record_yz_025.csv', 'data_record_yz_02.csv', 'data_record_yz_016.csv']\n",
        "# file_train_list_ = ['data_record_yz_03.csv', 'data_record_yz_02.csv']\n",
        "# file_vali_list_ = ['data_record_rand1200_hm0.csv']\n",
        "# file_test_list_ = ['data_record_rand1200_hm0.csv', 'data_record_rand1200_hm1.csv', 'data_record_rand1200_hm2.csv', 'data_record_rand1200_hm3.csv', 'data_record_rand1200_hm4.csv', 'data_record_rand1200_hm5.csv']\n",
        "\n",
        "# EXP4 Homing decay - WITH homing in training\n",
        "# file_train_list_ = ['data_record_yz_05_hm0.csv', 'data_record_yz_03_hm1.csv', 'data_record_yz_025_hm1.csv', 'data_record_yz_02_hm1.csv', 'data_record_yz_016_hm0.csv']\n",
        "# file_train_list_ = [ 'data_record_yz_03_hm1.csv', 'data_record_yz_016_hm0.csv']\n",
        "# file_vali_list_ = ['data_record_rand1200_hm0.csv']\n",
        "# file_test_list_ = ['data_record_rand1200_hm0.csv', 'data_record_rand1200_hm1.csv', 'data_record_rand1200_hm2.csv', 'data_record_rand1200_hm3.csv', 'data_record_rand1200_hm4.csv', 'data_record_rand1200_hm5.csv']\n",
        "\n",
        "#hidden_layers_ = [[1000], [500], [200]]\n",
        "# hidden_layers_ = [[600], [500], [400]]\n",
        "hidden_layers_ = [[100], [100]]\n",
        "learning_rate = 0.001  # original 0.0005\n",
        "# learning_rate = 0.0005  # original 0.0005\n",
        "batch_size = 1024\n",
        "epochs = 200\n",
        "activation = 'sigmoid'\n",
        "\n",
        "early_stop_restore_best = False\n",
        "\n",
        "kernel_regu_l1 = 0  #defult 1e-5, cur best 5e-5\n",
        "kernel_regu_l2 = 5e-4  #defult 1e-4, cur best 5e-4\n",
        "bias_regu = 0       #defult 1e-4, cur best 5e-4\n",
        "activity_regu = 0   #defult 1e-5, cur best 5e-5\n",
        "\n",
        "\n",
        "# left arm only ----------------------------\n",
        "feature_list = [#np.arange(12,13), # time stamp\n",
        "                np.arange(13,21), # jpos\n",
        "                # np.arange(29,30), # run level\n",
        "                # np.arange(30,31), # sub level\n",
        "                # np.arange(31,32), # last_seq\n",
        "                # np.arange(32,33), # type\n",
        "                # np.arange(34,37), # pos\n",
        "                # np.arange(40,49), # ori\n",
        "                # np.arange(58,67), # ord_d\n",
        "                # np.arange(76,79), # pos_d\n",
        "                # np.arange(82,98), # enc_val\n",
        "                # np.arange(98,106), # dac_val\n",
        "                np.arange(114,122), # tau\n",
        "                # np.arange(130,133),np.arange(133,138), # mpos\n",
        "                # np.arange(146,154), #  mvel\n",
        "                # np.arange(162,170), # jvel\n",
        "                # np.arange(178,186), # mpos_d\n",
        "                # np.arange(194,202), # jpos_d\n",
        "                # np.arange(210,211), # grasp_d\n",
        "                # np.arange(212,228), # enc_offset\n",
        "                # np.arange(228,234), # jac_vel\n",
        "                # np.arange(240,246) # jac_f\n",
        "                ]\n",
        "\n",
        "\n",
        "noise_type = 'no_noise'\n",
        "noise_list = [\n",
        "              ]\n",
        "\n",
        "feature_select = np.array([])\n",
        "for feature in feature_list:\n",
        "  feature_select = np.hstack([feature_select, feature])\n",
        "feature_select = feature_select.astype(int)\n",
        "feature_dim = feature_select.size\n",
        "\n",
        "noise_select = np.array([])\n",
        "for noise in noise_list:\n",
        "  noise_select = np.hstack([noise_select, noise])\n",
        "noise_select = noise_select.astype(int)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhfK8eW9_pPn"
      },
      "outputs": [],
      "source": [
        "# import data\n",
        "path_data = '/content/r2_learning_calibration/data/'\n",
        "\n",
        "\n",
        "# Load training data\n",
        "first_call = True\n",
        "for file_training in file_train_list_:\n",
        "  data = np.loadtxt(path_data + file_training, delimiter = ',')\n",
        "  if first_call:\n",
        "    data_train = data\n",
        "    first_call = False\n",
        "  else:\n",
        "    data_train = np.vstack((data_train, data))\n",
        "\n",
        "# Load testing data\n",
        "if combine_testset:\n",
        "  first_call = True\n",
        "  for file_test in file_test_list_:\n",
        "    data = np.loadtxt(path_data + file_test, delimiter = ',')\n",
        "    if first_call:\n",
        "      data_test = data\n",
        "      first_call = False\n",
        "    else:\n",
        "      data_test = np.vstack((data_test, data))\n",
        "  data_test_list = [data_test]\n",
        "else:\n",
        "  data_test_list = []\n",
        "  for file_test in file_test_list_:\n",
        "    data = np.loadtxt(path_data + file_test, delimiter = ',')\n",
        "    data_test_list.append(data)\n",
        "\n",
        "# load less validation data for faster training\n",
        "# data_vali = np.loadtxt(path_data + file_test_list_[0], delimiter = ',')\n",
        "# data_vali = data_vali[:10000,:]\n",
        "\n",
        "# Load vali data\n",
        "first_call = True\n",
        "for file_vali in file_vali_list_:\n",
        "  data = np.loadtxt(path_data + file_vali, delimiter = ',')\n",
        "  if first_call:\n",
        "    data_vali = data\n",
        "    first_call = False\n",
        "  else:\n",
        "    data_vali = np.vstack((data_vali, data))\n",
        "if use_short_vali == True:\n",
        "  data_vali = data_vali[:100,:]\n",
        "\n",
        "print('Training data loaded, shape:')\n",
        "print(data_train.shape)\n",
        "print('Validation data loaded, shape:')\n",
        "print(data_vali.shape)\n",
        "print('Test data loaded, shape:')\n",
        "for data_test in data_test_list:\n",
        "  print(data_test.shape)\n",
        "\n",
        "rad2deg = 180.0/np.pi\n",
        "deg2rad = np.pi/180.0\n",
        "\n",
        "# load the data for visualization only, joint 1 and 2 will be in deg, joint 3 in m\n",
        "time_line = data_train[:,0]\n",
        "ext_jpos_1 = data_train[:,1] *rad2deg   # joint pose from external joint encoders\n",
        "ext_jpos_2 = data_train[:,2] *rad2deg\n",
        "ext_jpos_3 = data_train[:,3]\n",
        "rs_jpos_1 = data_train[:,13]            # joint pose from ravenstate\n",
        "rs_jpos_2 = data_train[:,14]\n",
        "rs_jpos_3 = data_train[:,15] *deg2rad\n",
        "\n",
        "rs_jpos_1_lc = rs_jpos_1 + (np.mean(ext_jpos_1)-np.mean(rs_jpos_1))\n",
        "rs_jpos_2_lc = rs_jpos_2 + (np.mean(ext_jpos_2)-np.mean(rs_jpos_2))\n",
        "rs_jpos_3_lc = rs_jpos_3 + (np.mean(ext_jpos_3)-np.mean(rs_jpos_3))\n",
        "\n",
        "bias_jpos_1 = np.mean(ext_jpos_1)-np.mean(rs_jpos_1)\n",
        "bias_jpos_2 = np.mean(ext_jpos_2)-np.mean(rs_jpos_2)\n",
        "bias_jpos_3 = np.mean(ext_jpos_3)-np.mean(rs_jpos_3)\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.plot3D(ext_jpos_1, ext_jpos_2, ext_jpos_3)\n",
        "ax.plot3D(rs_jpos_1, rs_jpos_2, rs_jpos_3)\n",
        "plt.legend(('ext_enc','jpos_raven_state'))\n",
        "plt.title('Traning 3D traj')\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(time_line, ext_jpos_1)\n",
        "plt.plot(time_line, rs_jpos_1, 'r--')\n",
        "plt.legend(('ext_enc','jpos_raven_state'))\n",
        "plt.title('Joint 1 raw')\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(time_line, ext_jpos_2)\n",
        "plt.plot(time_line, rs_jpos_2, 'r--')\n",
        "plt.legend(('ext_enc','jpos_raven_state'))\n",
        "plt.title('Joint 2 raw')\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(time_line, ext_jpos_3)\n",
        "plt.plot(time_line, rs_jpos_3, 'r--')\n",
        "plt.legend(('ext_enc','jpos_raven_state'))\n",
        "plt.title('Joint 3 raw')\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(time_line, ext_jpos_1)\n",
        "plt.plot(time_line, rs_jpos_1_lc, 'r--')\n",
        "plt.legend(('ext_enc','jpos_raven_state'))\n",
        "plt.title('Joint 1 linear correct')\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(time_line, ext_jpos_2)\n",
        "plt.plot(time_line, rs_jpos_2_lc, 'r--')\n",
        "plt.legend(('ext_enc','jpos_raven_state'))\n",
        "plt.title('Joint 2 linear correct')\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(time_line, ext_jpos_3)\n",
        "plt.plot(time_line, rs_jpos_3_lc, 'r--')\n",
        "plt.legend(('ext_enc','jpos_raven_state'))\n",
        "plt.title('Joint 3 linear correct')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_0TuFQA4E67"
      },
      "outputs": [],
      "source": [
        "repeat_times = 5\n",
        "\n",
        "\n",
        "# Training\n",
        "x_train = data_train[:, feature_select]\n",
        "if 12 in feature_select:\n",
        "  x_train[:,0] = data_train[:, 0] # change the time stamp to relative time (start with 0)\n",
        "y_train = data_train[:, 1:4]\n",
        "\n",
        "ext_jpos_1 = data_train[:,1] *rad2deg   # joint pose from external joint encoders\n",
        "ext_jpos_2 = data_train[:,2] *rad2deg\n",
        "ext_jpos_3 = data_train[:,3]\n",
        "rs_jpos_1 = data_train[:,13]            # joint pose from ravenstate\n",
        "rs_jpos_2 = data_train[:,14]\n",
        "rs_jpos_3 = data_train[:,15] *deg2rad\n",
        "\n",
        "rs_jpos_1_lc = rs_jpos_1 + (np.mean(ext_jpos_1)-np.mean(rs_jpos_1))\n",
        "rs_jpos_2_lc = rs_jpos_2 + (np.mean(ext_jpos_2)-np.mean(rs_jpos_2))\n",
        "rs_jpos_3_lc = rs_jpos_3 + (np.mean(ext_jpos_3)-np.mean(rs_jpos_3))\n",
        "\n",
        "bias_jpos_1 = np.mean(ext_jpos_1)-np.mean(rs_jpos_1)\n",
        "bias_jpos_2 = np.mean(ext_jpos_2)-np.mean(rs_jpos_2)\n",
        "bias_jpos_3 = np.mean(ext_jpos_3)-np.mean(rs_jpos_3)\n",
        "\n",
        "if train_on_err:\n",
        "  y_train[:,0] = ext_jpos_1 - rs_jpos_1\n",
        "  y_train[:,1] = ext_jpos_2 - rs_jpos_2\n",
        "  y_train[:,2] = ext_jpos_3 - rs_jpos_3\n",
        "else:\n",
        "  y_train[:,0] = ext_jpos_1\n",
        "  y_train[:,1] = ext_jpos_2\n",
        "  y_train[:,2] = ext_jpos_3\n",
        "\n",
        "# scale the training set\n",
        "scaler_x_train = StandardScaler()\n",
        "scaler_y_train = StandardScaler()\n",
        "x_train_scaled = scaler_x_train.fit_transform(x_train)\n",
        "y_train_scaled = scaler_y_train.fit_transform(y_train)\n",
        "\n",
        "# Validation\n",
        "x_vali = data_vali[:, feature_select]\n",
        "if 12 in feature_select:\n",
        "  x_vali[:,0] = data_vali[:, 0] # change the time stamp to relative time (start with 0)\n",
        "y_vali = data_vali[:, 1:4]\n",
        "if train_on_err:\n",
        "  y_vali[:,0] = data_vali[:,1] *rad2deg - data_vali[:,13]\n",
        "  y_vali[:,1] = data_vali[:,2] *rad2deg - data_vali[:,14]\n",
        "  y_vali[:,2] = data_vali[:,3] - data_vali[:,15] *deg2rad\n",
        "else:\n",
        "  y_vali[:,0] = data_vali[:,1] *rad2deg\n",
        "  y_vali[:,1] = data_vali[:,2] *rad2deg\n",
        "  y_vali[:,2] = data_vali[:,3]\n",
        "x_vali_scaled = scaler_x_train.transform(x_vali)\n",
        "y_vali_scaled = scaler_y_train.transform(y_vali)\n",
        "\n",
        "\n",
        "x_test_list = []\n",
        "y_test_list = []\n",
        "time_line_list = []\n",
        "ext_jpos_1_list = []\n",
        "ext_jpos_2_list = []\n",
        "ext_jpos_3_list = []\n",
        "rs_jpos_1_list = []\n",
        "rs_jpos_2_list = []\n",
        "rs_jpos_3_list = []\n",
        "rs_jpos_1_lc_list = []\n",
        "rs_jpos_2_lc_list = []\n",
        "rs_jpos_3_lc_list = []\n",
        "for data_test in data_test_list:\n",
        "  time_line = data_test[:,0]\n",
        "  ext_jpos_1 = data_test[:,1] *rad2deg   # joint pose from external joint encoders\n",
        "  ext_jpos_2 = data_test[:,2] *rad2deg\n",
        "  ext_jpos_3 = data_test[:,3]\n",
        "\n",
        "  rs_jpos_1 = data_test[:,13]            # joint pose from ravenstate\n",
        "  rs_jpos_2 = data_test[:,14]\n",
        "  rs_jpos_3 = data_test[:,15] *deg2rad\n",
        "\n",
        "  rs_jpos_1_lc = rs_jpos_1 + bias_jpos_1\n",
        "  rs_jpos_2_lc = rs_jpos_2 + bias_jpos_2\n",
        "  rs_jpos_3_lc = rs_jpos_3 + bias_jpos_3\n",
        "\n",
        "\n",
        "  # add noise for ablation\n",
        "  for noise_idx in noise_select:\n",
        "    if noise_type == 'stdv':\n",
        "      data_test[:, noise_idx] = data_test[:, noise_idx] + np.random.default_rng().uniform(low=-np.std(data_test[:, noise_idx]), high=np.std(data_test[:, noise_idx]), size=data_test[:, noise_idx].shape)\n",
        "    if noise_type == 'mean':\n",
        "      data_test[:, noise_idx] = data_test[:, noise_idx] + np.random.default_rng().uniform(low=-0.5*np.mean(np.abs(data_test[:, noise_idx])), high=0.5*np.mean(np.abs(data_test[:, noise_idx])), size=data_test[:, noise_idx].shape)\n",
        "    if noise_type == 'zero':\n",
        "      data_test[:, noise_idx] = data_test[:, noise_idx] * 0\n",
        "    if noise_type == 'min-max':\n",
        "      range_idx = np.mean(heapq.nlargest(100 ,data_test[:, noise_idx])) - np.mean(heapq.nsmallest(100 ,data_test[:, noise_idx]))\n",
        "      data_test[:, noise_idx] = data_test[:, noise_idx] + np.random.default_rng().uniform(low=-0.5 * range_idx, high=0.5 * range_idx, size=data_test[:, noise_idx].shape)\n",
        "    if noise_type == 'temp':\n",
        "      data_test[:, noise_idx] = data_test[:, noise_idx] * -1\n",
        "    if noise_type == 'no_noise':\n",
        "      a = 1\n",
        "\n",
        "  x_test = data_test[:, feature_select]\n",
        "  if 12 in feature_select:\n",
        "    x_test[:,0] = data_test[:, 0] # change the time stamp to relative time (start with 0)\n",
        "  y_test = data_test[:, 1:4]\n",
        "\n",
        "\n",
        "  x_test_list.append(x_test)\n",
        "  y_test_list.append(y_test)\n",
        "  time_line_list.append(time_line)\n",
        "  ext_jpos_1_list.append(ext_jpos_1)\n",
        "  ext_jpos_2_list.append(ext_jpos_2)\n",
        "  ext_jpos_3_list.append(ext_jpos_3)\n",
        "  rs_jpos_1_list.append(rs_jpos_1)\n",
        "  rs_jpos_2_list.append(rs_jpos_2)\n",
        "  rs_jpos_3_list.append(rs_jpos_3)\n",
        "  rs_jpos_1_lc_list.append(rs_jpos_1_lc)\n",
        "  rs_jpos_2_lc_list.append(rs_jpos_2_lc)\n",
        "  rs_jpos_3_lc_list.append(rs_jpos_3_lc)\n",
        "\n",
        "\n",
        "list_finish_time = []\n",
        "list_infer_time = []\n",
        "list_rmse_j1_pdt = []\n",
        "list_rmse_j1_rs_lc = []\n",
        "list_rmse_j1_rs = []\n",
        "list_rmse_j2_pdt = []\n",
        "list_rmse_j2_rs_lc = []\n",
        "list_rmse_j2_rs = []\n",
        "list_rmse_j3_pdt = []\n",
        "list_rmse_j3_rs_lc = []\n",
        "list_rmse_j3_rs = []\n",
        "\n",
        "list_mxabse_j1_pdt = []\n",
        "list_mxabse_j1_rs_lc = []\n",
        "list_mxabse_j1_rs = []\n",
        "list_mxabse_j2_pdt = []\n",
        "list_mxabse_j2_rs_lc = []\n",
        "list_mxabse_j2_rs = []\n",
        "list_mxabse_j3_pdt = []\n",
        "list_mxabse_j3_rs_lc = []\n",
        "list_mxabse_j3_rs = []\n",
        "\n",
        "for i in range(0, repeat_times):\n",
        "  np.random.seed(10*i + 3)\n",
        "  tf.random.set_seed(10*i + 3)\n",
        "  tf.keras.utils.set_random_seed(10*i + 3)\n",
        "\n",
        "  dnn_model = None #clean the model\n",
        "########################################################################################################\n",
        "  class dnn_model():\n",
        "\n",
        "      folder_workspace = None\n",
        "      folder_NN = None\n",
        "\n",
        "      model = None\n",
        "\n",
        "\n",
        "      def set_workspace(self, folder):\n",
        "          self.folder_workspace = folder\n",
        "          self.folder_NN = self.folder_workspace + '/' + 'neural_network'\n",
        "          try:\n",
        "              os.mkdir(self.folder_workspace)\n",
        "          except:\n",
        "              _= 1\n",
        "          try:\n",
        "              os.mkdir(self.folder_NN)\n",
        "          except:\n",
        "              _= 1\n",
        "          return None\n",
        "\n",
        "      def build_model(self, dim_input, dim_output, hidden_layers_ = [[200],[150],[100]], learning_rate = 0.001):\n",
        "          self.model = Sequential()\n",
        "\n",
        "          # input layer\n",
        "          self.model.add(tf.keras.Input(shape=(dim_input,)))\n",
        "\n",
        "          ## add layer normalization\n",
        "          # self.model.add(tf.keras.layers.LayerNormalization(\n",
        "          #                 axis=0,                            # normalize according to columns\n",
        "          #                 epsilon=0.001,\n",
        "          #                 center=True,\n",
        "          #                 scale=True,\n",
        "          #                 beta_initializer=\"zeros\",\n",
        "          #                 gamma_initializer=\"ones\",\n",
        "          #                 beta_regularizer=None,\n",
        "          #                 gamma_regularizer=None,\n",
        "          #                 beta_constraint=None,\n",
        "          #                 gamma_constraint=None,))\n",
        "\n",
        "          # # add batch normalization\n",
        "          # self.model.add(tf.keras.layers.BatchNormalization(\n",
        "          #                 axis=-1,\n",
        "          #                 momentum=0.99,\n",
        "          #                 epsilon=0.001,\n",
        "          #                 center=True,\n",
        "          #                 scale=True,\n",
        "          #                 beta_initializer=\"zeros\",\n",
        "          #                 gamma_initializer=\"ones\",\n",
        "          #                 moving_mean_initializer=\"zeros\",\n",
        "          #                 moving_variance_initializer=\"ones\",\n",
        "          #                 beta_regularizer=None,\n",
        "          #                 gamma_regularizer=None,\n",
        "          #                 beta_constraint=None,\n",
        "          #                 gamma_constraint=None,\n",
        "          #                 renorm=False,\n",
        "          #                 renorm_clipping=None,\n",
        "          #                 renorm_momentum=0.99,\n",
        "          #                 fused=None,\n",
        "          #                 trainable=True,\n",
        "          #                 virtual_batch_size=None,\n",
        "          #                 adjustment=None,\n",
        "          #                 name=None,))\n",
        "\n",
        "          # hidden layer\n",
        "          for hidden_layer in hidden_layers_:\n",
        "              self.model.add(Dense(units = hidden_layer[0],\n",
        "                              activation= activation,\n",
        "                              use_bias=True,\n",
        "                              kernel_initializer=\"glorot_uniform\",\n",
        "                              bias_initializer=\"zeros\",\n",
        "                              kernel_regularizer=regularizers.L1L2(l1=kernel_regu_l1, l2=kernel_regu_l2),\n",
        "                              bias_regularizer=regularizers.L2(bias_regu),\n",
        "                              activity_regularizer=regularizers.L2(activity_regu),\n",
        "                              kernel_constraint=None,\n",
        "                              bias_constraint=None,))\n",
        "\n",
        "          # output layer\n",
        "          self.model.add(Dense(units = dim_output,\n",
        "                              activation='linear',\n",
        "                              use_bias=True,\n",
        "                              kernel_initializer=\"glorot_uniform\",\n",
        "                              bias_initializer=\"zeros\",\n",
        "                              kernel_regularizer=None,\n",
        "                              bias_regularizer=None,\n",
        "                              activity_regularizer=None,\n",
        "                              kernel_constraint=None,\n",
        "                              bias_constraint=None,))\n",
        "\n",
        "          optimizer_method = tf.keras.optimizers.Adam(learning_rate=learning_rate,\n",
        "                                  beta_1=0.9,\n",
        "                                  beta_2=0.999,\n",
        "                                  epsilon=1e-07,\n",
        "                                  amsgrad=False,\n",
        "                                  name=\"Adam\")\n",
        "\n",
        "          # loss_function = tf.keras.losses.LogCosh(reduction=\"auto\", name=\"log_cosh\")\n",
        "\n",
        "          metrics_function = [tf.keras.metrics.MeanAbsoluteError(name=\"mean_absolute_error\", dtype=None),\n",
        "                              tf.keras.metrics.MeanAbsolutePercentageError(name=\"mean_absolute_percentage_error\", dtype=None)]\n",
        "\n",
        "          self.model.compile(optimizer = optimizer_method,\n",
        "                            loss = 'mse',\n",
        "                            metrics = metrics_function,\n",
        "                            loss_weights=None,\n",
        "                            weighted_metrics=None,\n",
        "                            run_eagerly=None)\n",
        "          return None\n",
        "\n",
        "      def train_model(self, X, y, batch_size, epochs, validation_data=None, early_stop_min_delta = 0, early_stop_patience = 5):\n",
        "\n",
        "          early_stop = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",\n",
        "                                                          min_delta = early_stop_min_delta,\n",
        "                                                          patience = early_stop_patience,\n",
        "                                                          verbose=1,\n",
        "                                                          mode=\"auto\",\n",
        "                                                          baseline=None,\n",
        "                                                          restore_best_weights=early_stop_restore_best)\n",
        "\n",
        "          History = self.model.fit(x = X,\n",
        "                                  y = y,\n",
        "                                  batch_size=batch_size,\n",
        "                                  epochs = epochs,\n",
        "                                  verbose=0,\n",
        "                                  callbacks=[early_stop],\n",
        "                                  validation_split=0.01,\n",
        "                                  validation_data = validation_data,\n",
        "                                  shuffle=True,\n",
        "                                  class_weight=None,\n",
        "                                  sample_weight=None,\n",
        "                                  initial_epoch=0,\n",
        "                                  steps_per_epoch=None,\n",
        "                                  validation_steps=None,\n",
        "                                  validation_batch_size=None,\n",
        "                                  validation_freq=1,\n",
        "                                  max_queue_size=10,\n",
        "                                  workers=1,\n",
        "                                  use_multiprocessing=False)\n",
        "\n",
        "          return History\n",
        "\n",
        "      def evaluate_model(self, X, y):\n",
        "          test_result = self.model.evaluate(x = X,\n",
        "                                              y = y,\n",
        "                                              batch_size=None,\n",
        "                                              verbose=1,\n",
        "                                              sample_weight=None,\n",
        "                                              steps=None,\n",
        "                                              callbacks=None,\n",
        "                                              max_queue_size=10,\n",
        "                                              workers=1,\n",
        "                                              use_multiprocessing=False,\n",
        "                                              return_dict=True)\n",
        "\n",
        "          return test_result\n",
        "\n",
        "      def save_model(self, file_path = None):\n",
        "          if file_path == None:\n",
        "              file_path = self.folder_NN + '/' + 'model.h5'\n",
        "\n",
        "          self.model.save(filepath = file_path,\n",
        "                          overwrite=True,\n",
        "                          include_optimizer=True,\n",
        "                          save_format=None,\n",
        "                          signatures=None,\n",
        "                          options=None)\n",
        "          return None\n",
        "\n",
        "      def load_model(self, file_path = None):\n",
        "          if file_path == None:\n",
        "              file_path = self.folder_NN + '/' + 'model.h5'\n",
        "\n",
        "          self.model = tf.keras.models.load_model(filepath = file_path)\n",
        "          return None\n",
        "########################################################################################################\n",
        "\n",
        "  start_time = time.time()\n",
        "  dnn_model = dnn_model()\n",
        "\n",
        "  dnn_model.set_workspace('r2_learning_calibration/r2_cali_model')\n",
        "\n",
        "  # dnn_model.build_model(dim_input = 17, dim_output = 3, hidden_layers_ = [[600],[500],[400]], learning_rate = 0.001)\n",
        "  dnn_model.build_model(dim_input = feature_dim, dim_output = 3, hidden_layers_ = hidden_layers_, learning_rate = learning_rate)\n",
        "\n",
        "  dnn_model.model.summary()\n",
        "\n",
        "  train_history = dnn_model.train_model(X = x_train_scaled,\n",
        "                                        y = y_train_scaled,\n",
        "                                        batch_size = batch_size,\n",
        "                                        epochs = epochs,\n",
        "                                        validation_data = (x_vali_scaled, y_vali_scaled),\n",
        "                                        early_stop_min_delta = 0,\n",
        "                                        early_stop_patience = 1000)\n",
        "  finish_time = time.time() - start_time\n",
        "  test_result = dnn_model.evaluate_model(X = x_train_scaled, y = y_train_scaled)\n",
        "  print(test_result)\n",
        "\n",
        "  dnn_model.save_model(file_path = None)\n",
        "\n",
        "\n",
        "  print('Training time:')\n",
        "  print(str(time.time() - start_time))\n",
        "  # summarize history for loss\n",
        "  plt.figure()\n",
        "  plt.ylim((0,0.5))\n",
        "  plt.plot(train_history.history['loss'])\n",
        "  plt.plot(train_history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'vali'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  rmse_j1_pdt_lst = []\n",
        "  rmse_j2_pdt_lst = []\n",
        "  rmse_j3_pdt_lst = []\n",
        "  rmse_j1_rs_lst = []\n",
        "  rmse_j2_rs_lst = []\n",
        "  rmse_j3_rs_lst = []\n",
        "  rmse_j1_rs_lc_lst = []\n",
        "  rmse_j2_rs_lc_lst = []\n",
        "  rmse_j3_rs_lc_lst = []\n",
        "\n",
        "  mxabse_j1_pdt_lst = []\n",
        "  mxabse_j2_pdt_lst = []\n",
        "  mxabse_j3_pdt_lst = []\n",
        "  mxabse_j1_rs_lst = []\n",
        "  mxabse_j2_rs_lst = []\n",
        "  mxabse_j3_rs_lst = []\n",
        "  mxabse_j1_rs_lc_lst = []\n",
        "  mxabse_j2_rs_lc_lst = []\n",
        "  mxabse_j3_rs_lc_lst = []\n",
        "\n",
        "  idx = 0\n",
        "  for x_test in x_test_list:\n",
        "    y_test = y_test_list[idx]\n",
        "    time_line = time_line_list[idx]\n",
        "    ext_jpos_1 = ext_jpos_1_list[idx]\n",
        "    ext_jpos_2 = ext_jpos_2_list[idx]\n",
        "    ext_jpos_3 = ext_jpos_3_list[idx]\n",
        "    rs_jpos_1 = rs_jpos_1_list[idx]\n",
        "    rs_jpos_2 = rs_jpos_2_list[idx]\n",
        "    rs_jpos_3 = rs_jpos_3_list[idx]\n",
        "    rs_jpos_1_lc = rs_jpos_1_lc_list[idx]\n",
        "    rs_jpos_2_lc = rs_jpos_2_lc_list[idx]\n",
        "    rs_jpos_3_lc = rs_jpos_3_lc_list[idx]\n",
        "    idx += 1\n",
        "  # Evaluate and plot result -----------------------------------------------------\n",
        "    y_predict = dnn_model.model.predict(scaler_x_train.transform(x_test))\n",
        "    y_predict = scaler_y_train.inverse_transform(y_predict)\n",
        "    print(y_predict.shape)\n",
        "\n",
        "    if train_on_err:\n",
        "      pdt_jpos_1 = rs_jpos_1 + y_predict[:,0]\n",
        "      pdt_jpos_2 = rs_jpos_2 + y_predict[:,1]\n",
        "      pdt_jpos_3 = rs_jpos_3 + y_predict[:,2]\n",
        "    else:\n",
        "      pdt_jpos_1 = y_predict[:,0]\n",
        "      pdt_jpos_2 = y_predict[:,1]\n",
        "      pdt_jpos_3 = y_predict[:,2]\n",
        "\n",
        "    if save_rst:\n",
        "      pdt_jpos = np.zeros(y_predict.shape)\n",
        "      pdt_jpos[:,0] = pdt_jpos_1\n",
        "      pdt_jpos[:,1] = pdt_jpos_2\n",
        "      pdt_jpos[:,2] = pdt_jpos_3\n",
        "      np.savetxt('dnn_rst_'+str(idx)+'.csv', pdt_jpos)\n",
        "\n",
        "    rmse_j1_pdt = np.sqrt(np.mean(np.square(pdt_jpos_1 - ext_jpos_1)))\n",
        "    rmse_j2_pdt = np.sqrt(np.mean(np.square(pdt_jpos_2 - ext_jpos_2)))\n",
        "    rmse_j3_pdt = np.sqrt(np.mean(np.square(pdt_jpos_3 - ext_jpos_3)))\n",
        "    rmse_j1_rs = np.sqrt(np.mean(np.square(rs_jpos_1 - ext_jpos_1)))\n",
        "    rmse_j2_rs = np.sqrt(np.mean(np.square(rs_jpos_2 - ext_jpos_2)))\n",
        "    rmse_j3_rs = np.sqrt(np.mean(np.square(rs_jpos_3 - ext_jpos_3)))\n",
        "    rmse_j1_rs_lc = np.sqrt(np.mean(np.square(rs_jpos_1_lc - ext_jpos_1)))\n",
        "    rmse_j2_rs_lc = np.sqrt(np.mean(np.square(rs_jpos_2_lc - ext_jpos_2)))\n",
        "    rmse_j3_rs_lc = np.sqrt(np.mean(np.square(rs_jpos_3_lc - ext_jpos_3)))\n",
        "\n",
        "    mxabse_j1_pdt = np.max(np.abs(pdt_jpos_1 - ext_jpos_1))\n",
        "    mxabse_j2_pdt = np.max(np.abs(pdt_jpos_2 - ext_jpos_2))\n",
        "    mxabse_j3_pdt = np.max(np.abs(pdt_jpos_3 - ext_jpos_3))\n",
        "    mxabse_j1_rs = np.max(np.abs(rs_jpos_1 - ext_jpos_1))\n",
        "    mxabse_j2_rs = np.max(np.abs(rs_jpos_2 - ext_jpos_2))\n",
        "    mxabse_j3_rs = np.max(np.abs(rs_jpos_3 - ext_jpos_3))\n",
        "    mxabse_j1_rs_lc = np.max(np.abs(rs_jpos_1_lc - ext_jpos_1))\n",
        "    mxabse_j2_rs_lc = np.max(np.abs(rs_jpos_2_lc - ext_jpos_2))\n",
        "    mxabse_j3_rs_lc = np.max(np.abs(rs_jpos_3_lc - ext_jpos_3))\n",
        "\n",
        "    rmse_j1_pdt_lst.append(rmse_j1_pdt)\n",
        "    rmse_j2_pdt_lst.append(rmse_j2_pdt)\n",
        "    rmse_j3_pdt_lst.append(rmse_j3_pdt)\n",
        "    rmse_j1_rs_lst.append(rmse_j1_rs)\n",
        "    rmse_j2_rs_lst.append(rmse_j2_rs)\n",
        "    rmse_j3_rs_lst.append(rmse_j3_rs)\n",
        "    rmse_j1_rs_lc_lst.append(rmse_j1_rs_lc)\n",
        "    rmse_j2_rs_lc_lst.append(rmse_j2_rs_lc)\n",
        "    rmse_j3_rs_lc_lst.append(rmse_j3_rs_lc)\n",
        "\n",
        "    mxabse_j1_pdt_lst.append(mxabse_j1_pdt)\n",
        "    mxabse_j2_pdt_lst.append(mxabse_j2_pdt)\n",
        "    mxabse_j3_pdt_lst.append(mxabse_j3_pdt)\n",
        "    mxabse_j1_rs_lst.append(mxabse_j1_rs)\n",
        "    mxabse_j2_rs_lst.append(mxabse_j2_rs)\n",
        "    mxabse_j3_rs_lst.append(mxabse_j3_rs)\n",
        "    mxabse_j1_rs_lc_lst.append(mxabse_j1_rs_lc)\n",
        "    mxabse_j2_rs_lc_lst.append(mxabse_j2_rs_lc)\n",
        "    mxabse_j3_rs_lc_lst.append(mxabse_j3_rs_lc)\n",
        "\n",
        "  # testing inference time------------------------------------------------------------------\n",
        "  # this generator is only for testing the inference time\n",
        "  infer_start_time = time.time()\n",
        "  infer_No = 5000\n",
        "  multi_infer = 8\n",
        "  for i in range(infer_No):\n",
        "    if multi_infer != 1:\n",
        "      _ = dnn_model.model.predict(scaler_x_train.transform(x_test[i:i+multi_infer,:]), verbose=0)\n",
        "    else:\n",
        "      _ = dnn_model.model.predict(scaler_x_train.transform([x_test[i]]), verbose=0)\n",
        "  list_infer_time.append((time.time() - infer_start_time)/(infer_No*multi_infer))\n",
        "  #-----------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "  list_finish_time.append(finish_time)\n",
        "  list_rmse_j1_pdt.append(rmse_j1_pdt_lst)\n",
        "  list_rmse_j1_rs_lc.append(rmse_j1_rs_lc_lst)\n",
        "  list_rmse_j1_rs.append(rmse_j1_rs_lst)\n",
        "  list_rmse_j2_pdt.append(rmse_j2_pdt_lst)\n",
        "  list_rmse_j2_rs_lc.append(rmse_j2_rs_lc_lst)\n",
        "  list_rmse_j2_rs.append(rmse_j2_rs_lst)\n",
        "  list_rmse_j3_pdt.append(rmse_j3_pdt_lst)\n",
        "  list_rmse_j3_rs_lc.append(rmse_j3_rs_lc_lst)\n",
        "  list_rmse_j3_rs.append(rmse_j3_rs_lst)\n",
        "\n",
        "  list_mxabse_j1_pdt.append(mxabse_j1_pdt_lst)\n",
        "  list_mxabse_j1_rs_lc.append(mxabse_j1_rs_lc_lst)\n",
        "  list_mxabse_j1_rs.append(mxabse_j1_rs_lst)\n",
        "  list_mxabse_j2_pdt.append(mxabse_j2_pdt_lst)\n",
        "  list_mxabse_j2_rs_lc.append(mxabse_j2_rs_lc_lst)\n",
        "  list_mxabse_j2_rs.append(mxabse_j2_rs_lst)\n",
        "  list_mxabse_j3_pdt.append(mxabse_j3_pdt_lst)\n",
        "  list_mxabse_j3_rs_lc.append(mxabse_j3_rs_lc_lst)\n",
        "  list_mxabse_j3_rs.append(mxabse_j3_rs_lst)\n",
        "\n",
        "  print('-------------------')\n",
        "  print('Learning Calibration RMSE Joint 1 (deg):')\n",
        "  print(rmse_j1_pdt_lst)\n",
        "  print('ravenstate RMSE Joint 1 linear corrected (deg):')\n",
        "  print(rmse_j1_rs_lc_lst)\n",
        "  print('ravenstate RMSE Joint 1 (deg):')\n",
        "  print(rmse_j1_rs_lst)\n",
        "  print('-------------------')\n",
        "  print('Learning Calibration RMSE Joint 2 (deg):')\n",
        "  print(rmse_j2_pdt_lst)\n",
        "  print('ravenstate RMSE Joint 2 linear corrected (deg):')\n",
        "  print(rmse_j2_rs_lc_lst)\n",
        "  print('ravenstate RMSE Joint 2 (deg):')\n",
        "  print(rmse_j2_rs_lst)\n",
        "  print('-------------------')\n",
        "  print('Learning Calibration RMSE Joint 3 (mm):')\n",
        "  print(np.array(rmse_j3_pdt_lst) * 1000)\n",
        "  print('ravenstate RMSE Joint 3 linear corrected (mm):')\n",
        "  print(np.array(rmse_j3_rs_lc_lst) * 1000)\n",
        "  print('ravenstate RMSE Joint 3 (mm):')\n",
        "  print(np.array(rmse_j3_rs_lst) * 1000)\n",
        "  print('-------------------')\n",
        "\n",
        "  # fig = plt.figure()\n",
        "  # plt.plot(time_line, ext_jpos_1, 'k')\n",
        "  # plt.plot(time_line, rs_jpos_1, 'b--')\n",
        "  # plt.plot(time_line, pdt_jpos_1, 'r--')\n",
        "  # plt.legend(('gt', 'ravenstate', 'learn_cali'))\n",
        "  # plt.title('Joint 1')\n",
        "\n",
        "  # fig = plt.figure()\n",
        "  # plt.plot(time_line, ext_jpos_2, 'k')\n",
        "  # plt.plot(time_line, rs_jpos_2, 'b--')\n",
        "  # plt.plot(time_line, pdt_jpos_2, 'r--')\n",
        "  # plt.legend(('gt', 'ravenstate', 'learn_cali'))\n",
        "  # plt.title('Joint 2')\n",
        "\n",
        "  # fig = plt.figure()\n",
        "  # plt.plot(time_line, ext_jpos_3, 'k')\n",
        "  # plt.plot(time_line, rs_jpos_3, 'b--')\n",
        "  # plt.plot(time_line, pdt_jpos_3, 'r--')\n",
        "  # plt.legend(('gt', 'ravenstate', 'learn_cali'))\n",
        "  # plt.title('Joint 3')\n",
        "\n",
        "# print final result ----------------------------------------------------------\n",
        "print('################################################################################')\n",
        "print('Final results:')\n",
        "print('Finish time:')\n",
        "print(np.mean(list_finish_time))\n",
        "print(list_finish_time)\n",
        "print('Inference time:')\n",
        "print(np.mean(list_infer_time))\n",
        "print(list_infer_time)\n",
        "print('-----------------------------------------------')\n",
        "print('Learning Calibration RMSE Joint 1 (deg):')\n",
        "arr_rmse_j1_pdt = np.array(list_rmse_j1_pdt)\n",
        "for i in range(0,arr_rmse_j1_pdt.shape[1]):\n",
        "  print(np.mean(arr_rmse_j1_pdt[:,i]))\n",
        "print('Learning Calibration SD Joint 1 (deg):')\n",
        "arr_rmse_j1_pdt = np.array(list_rmse_j1_pdt)\n",
        "for i in range(0,arr_rmse_j1_pdt.shape[1]):\n",
        "  print(np.std(arr_rmse_j1_pdt[:,i]))\n",
        "print('Learning Calibration MAX Abs Error Joint 1 (deg):')\n",
        "arr_mxabse_j1_pdt = np.array(list_mxabse_j1_pdt)\n",
        "for i in range(0,arr_mxabse_j1_pdt.shape[1]):\n",
        "  print(np.mean(arr_mxabse_j1_pdt[:,i]))\n",
        "\n",
        "print('ravenstate RMSE Joint 1 linear corrected (deg):')\n",
        "# print(np.mean(list_rmse_j1_rs_lc))\n",
        "arr_rmse_j1_rs_lc = np.array(list_rmse_j1_rs_lc)\n",
        "for i in range(0,arr_rmse_j1_rs_lc.shape[1]):\n",
        "  print(np.mean(arr_rmse_j1_rs_lc[:,i]))\n",
        "print('ravenstate MAX Abs Error Joint 1 linear corrected (deg):')\n",
        "# print(np.mean(list_rmse_j1_rs_lc))\n",
        "arr_mxabse_j1_rs_lc = np.array(list_mxabse_j1_rs_lc)\n",
        "for i in range(0,arr_mxabse_j1_rs_lc.shape[1]):\n",
        "  print(np.max(np.abs(arr_mxabse_j1_rs_lc[:,i])))\n",
        "\n",
        "\n",
        "print('ravenstate RMSE Joint 1 (deg):')\n",
        "# print(np.mean(list_rmse_j1_rs))\n",
        "arr_rmse_j1_rs = np.array(list_rmse_j1_rs)\n",
        "for i in range(0,arr_rmse_j1_rs.shape[1]):\n",
        "  print(np.mean(arr_rmse_j1_rs[:,i]))\n",
        "print('ravenstate MAX Abs Error Joint 1 (deg):')\n",
        "# print(np.mean(list_rmse_j1_rs))\n",
        "arr_mxabse_j1_rs = np.array(list_mxabse_j1_rs)\n",
        "for i in range(0,arr_mxabse_j1_rs.shape[1]):\n",
        "  print(np.max(np.abs(arr_mxabse_j1_rs[:,i])))\n",
        "\n",
        "print('-----------------------------------------------')\n",
        "print('Learning Calibration RMSE Joint 2 (deg):')\n",
        "# print(np.mean(list_rmse_j2_pdt))\n",
        "arr_rmse_j2_pdt = np.array(list_rmse_j2_pdt)\n",
        "for i in range(0,arr_rmse_j2_pdt.shape[1]):\n",
        "  print(np.mean(arr_rmse_j2_pdt[:,i]))\n",
        "print('Learning Calibration SD Joint 2 (deg):')\n",
        "# print(np.mean(list_rmse_j2_pdt))\n",
        "arr_rmse_j2_pdt = np.array(list_rmse_j2_pdt)\n",
        "for i in range(0,arr_rmse_j2_pdt.shape[1]):\n",
        "  print(np.std(arr_rmse_j2_pdt[:,i]))\n",
        "print('Learning Calibration MAX Abs Error Joint 2 (deg):')\n",
        "arr_mxabse_j2_pdt = np.array(list_mxabse_j2_pdt)\n",
        "for i in range(0,arr_mxabse_j2_pdt.shape[1]):\n",
        "  print(np.max(np.abs(arr_mxabse_j2_pdt[:,i])))\n",
        "\n",
        "print('ravenstate RMSE Joint 2 linear corrected (deg):')\n",
        "# print(np.mean(list_rmse_j2_rs_lc))\n",
        "arr_rmse_j2_rs_lc = np.array(list_rmse_j2_rs_lc)\n",
        "for i in range(0,arr_rmse_j2_rs_lc.shape[1]):\n",
        "  print(np.mean(arr_rmse_j2_rs_lc[:,i]))\n",
        "print('ravenstate MAX Abs Error Joint 2 linear corrected (deg):')\n",
        "# print(np.mean(list_rmse_j2_rs_lc))\n",
        "arr_mxabse_j2_rs_lc = np.array(list_mxabse_j2_rs_lc)\n",
        "for i in range(0,arr_mxabse_j2_rs_lc.shape[1]):\n",
        "  print(np.max(np.abs(arr_mxabse_j2_rs_lc[:,i])))\n",
        "\n",
        "\n",
        "print('ravenstate RMSE Joint 2 (deg):')\n",
        "# print(np.mean(list_rmse_j2_rs))\n",
        "arr_rmse_j2_rs = np.array(list_rmse_j2_rs)\n",
        "for i in range(0,arr_rmse_j2_rs.shape[1]):\n",
        "  print(np.mean(arr_rmse_j2_rs[:,i]))\n",
        "print('ravenstate MAX Abs Error Joint 2 (deg):')\n",
        "# print(np.mean(list_rmse_j2_rs))\n",
        "arr_mxabse_j2_rs = np.array(list_mxabse_j2_rs)\n",
        "for i in range(0,arr_mxabse_j2_rs.shape[1]):\n",
        "  print(np.max(np.abs(arr_mxabse_j2_rs[:,i])))\n",
        "\n",
        "print('-----------------------------------------------')\n",
        "print('Learning Calibration RMSE Joint 3 (mm):')\n",
        "# print(np.mean(list_rmse_j3_pdt))\n",
        "arr_rmse_j3_pdt = np.array(list_rmse_j3_pdt)\n",
        "for i in range(0,arr_rmse_j3_pdt.shape[1]):\n",
        "  print(np.mean(arr_rmse_j3_pdt[:,i]) * 1000)\n",
        "print('Learning Calibration SD Joint 3 (mm):')\n",
        "# print(np.mean(list_rmse_j3_pdt))\n",
        "arr_rmse_j3_pdt = np.array(list_rmse_j3_pdt)\n",
        "for i in range(0,arr_rmse_j3_pdt.shape[1]):\n",
        "  print(np.std(arr_rmse_j3_pdt[:,i]) * 1000)\n",
        "print('Learning Calibration MAX Abs Error Joint 3 (deg):')\n",
        "arr_mxabse_j3_pdt = np.array(list_mxabse_j3_pdt)\n",
        "for i in range(0,arr_mxabse_j3_pdt.shape[1]):\n",
        "  print(np.max(np.abs(arr_mxabse_j3_pdt[:,i])) *1000)\n",
        "\n",
        "print('ravenstate RMSE Joint 3 linear corrected (mm):')\n",
        "# print(np.mean(list_rmse_j3_rs_lc))\n",
        "arr_rmse_j3_rs_lc = np.array(list_rmse_j3_rs_lc)\n",
        "for i in range(0,arr_rmse_j3_rs_lc.shape[1]):\n",
        "  print(np.mean(arr_rmse_j3_rs_lc[:,i]) * 1000)\n",
        "print('ravenstate MAX Abs Error Joint 3 linear corrected (mm):')\n",
        "# print(np.mean(list_rmse_j3_rs_lc))\n",
        "arr_mxabse_j3_rs_lc = np.array(list_mxabse_j3_rs_lc)\n",
        "for i in range(0,arr_mxabse_j3_rs_lc.shape[1]):\n",
        "  print(np.max(np.abs(arr_mxabse_j3_rs_lc[:,i])) *1000)\n",
        "\n",
        "print('ravenstate RMSE Joint 3 (mm):')\n",
        "# print(np.mean(list_rmse_j3_rs))\n",
        "arr_rmse_j3_rs = np.array(list_rmse_j3_rs)\n",
        "for i in range(0,arr_rmse_j3_rs.shape[1]):\n",
        "  print(np.mean(arr_rmse_j3_rs[:,i]) * 1000)\n",
        "print('ravenstate MAX Abs Error Joint 3 (mm):')\n",
        "# print(np.mean(list_rmse_j3_rs))\n",
        "arr_mxabse_j3_rs = np.array(list_mxabse_j3_rs)\n",
        "for i in range(0,arr_mxabse_j3_rs.shape[1]):\n",
        "  print(np.max(np.abs(arr_mxabse_j3_rs[:,i])) *1000)\n",
        "\n",
        "\n",
        "print('||||||||||||||||||||||||||||||||||||||||||||||||')\n",
        "print('Training Parameters: ')\n",
        "print('Hidden layers: ', hidden_layers_)\n",
        "print('learning_rate:', learning_rate)\n",
        "print('batch_size', batch_size)\n",
        "print('epochs', epochs)\n",
        "print('activation: ', activation)\n",
        "print('kernel_regu_l1', kernel_regu_l1)\n",
        "print('kernel_regu_l2', kernel_regu_l2)\n",
        "print('bias_regu', bias_regu)\n",
        "print('activity_regu', activity_regu)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}